{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "import helper\n",
    "from feature_optimization import FeatureOptimizer\n",
    "import feature_opt_functions as funcs\n",
    "from indices import *\n",
    "\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import jaccard_score, average_precision_score\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(92, 4)\n",
      "(92, 4)\n"
     ]
    }
   ],
   "source": [
    "dictOfStressedImageNames = {}\n",
    "for v in helper.getStressedImagesNames('stress_date.xlsx'):\n",
    "    if v is not None:\n",
    "        dictOfStressedImageNames[v[0]] = v[1]\n",
    "\n",
    "dictOfReferenceImageNames = {}\n",
    "for v in helper.getStressedImagesNames('reference_date.xlsx'):\n",
    "    if v is not None:\n",
    "        dictOfReferenceImageNames[v[0]] = v[1]\n",
    "\n",
    "tilesPerGroup = None\n",
    "with open(os.path.join(\"subdivs\", \"tiles_2img_2.json\"), 'r') as fout:\n",
    "    tilesPerGroup = json.load(fout)\n",
    "\n",
    "train_tiles = tilesPerGroup[0]\n",
    "test_tiles = tilesPerGroup[1]\n",
    "\n",
    "train_data = helper.getH_and_S(dictOfStressedImageNames, train_tiles)\n",
    "train_data_ref = helper.getH_and_S(dictOfReferenceImageNames, train_tiles)\n",
    "test_data = helper.getH_and_S(dictOfStressedImageNames, test_tiles)\n",
    "test_data_ref = helper.getH_and_S(dictOfReferenceImageNames, test_tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateModel(informativeness_treshold, independency_treshold):\n",
    "    # BEGIN FEATURE OPTIMIZER BLOCK\n",
    "    print(\"-\"*33)\n",
    "    print(\"START NEW MODEL\")\n",
    "    print(\"INFO:\", informativeness_treshold, \"INDEP:\", independency_treshold)\n",
    "    print(\"Feature Optimization\")\n",
    "    args = { \n",
    "        \"num_generations\":150, \n",
    "        \"num_parents_mating\":2,\n",
    "        \"parent_selection_type\":\"sss\",\n",
    "        \"crossover_type\":\"uniform\",\n",
    "        \"keep_parents\":1,\n",
    "        \"keep_elitism\":1,\n",
    "        \"sol_per_pop\":150,\n",
    "\t\t\"mutation_probability\":0.3,\n",
    "        \"parallel_processing\":8\n",
    "        }\n",
    "    \n",
    "    opt = FeatureOptimizer(IndicesClassEncoder([NORMP4], 12),\n",
    "                           12, \n",
    "                            funcs.bhattacharyya_distance, \n",
    "                            funcs.spearman_independency, \n",
    "                            optimization_method=\"genetic\", \n",
    "                            optimizer_args=args,\n",
    "                            informativeness_threshold=informativeness_treshold, \n",
    "                            independency_threshold=independency_treshold)\n",
    "    \n",
    "    data = [None]*2\n",
    "    data[0] = helper.leaveFinite(train_data[0])\n",
    "    data[1] = helper.leaveFinite(train_data[1])\n",
    "    opt.fit(data, data[1])\n",
    "    # END FEATURE OPTIMIZER BLOCK\n",
    "\n",
    "    # BEGIN SCALING & DATA PREPARATION BLOCK\n",
    "    print(\"Data Preparation\")\n",
    "    indices_train_H = helper.leaveFinite(opt.transform_series([train_data[0], train_data_ref[0]])).swapaxes(0, 1)\n",
    "    indices_train_S = helper.leaveFinite(opt.transform_series([train_data[1], train_data_ref[1]])).swapaxes(0, 1)\n",
    "    indices_test_H = helper.leaveFinite(opt.transform_series([test_data[0], test_data_ref[0]])).swapaxes(0, 1)\n",
    "    indices_test_S = helper.leaveFinite(opt.transform_series([test_data[1], test_data_ref[1]])).swapaxes(0, 1)\n",
    "\n",
    "    scaler = RobustScaler(unit_variance=True)\n",
    "\n",
    "    indices_train_H = scaler.fit_transform(indices_train_H)\n",
    "    indices_train_S = scaler.transform(indices_train_S)\n",
    "    indices_test_H = scaler.transform(indices_test_H)\n",
    "    indices_test_S = scaler.transform(indices_test_S)\n",
    "\n",
    "    # oversample to make training set balanced\n",
    "    # ros = RandomOverSampler(random_state=15151515)\n",
    "    train_X, train_y = helper.joinData(indices_train_H, indices_train_S)\n",
    "    # train_X, train_y = ros.fit_resample(train_X, train_y)\n",
    "\n",
    "    test_X, test_y = helper.joinData(indices_test_H, indices_test_S)\n",
    "    # END SCALING & DATA PREPARATION BLOCK\n",
    "\n",
    "    # BEGIN MODEL TRAINING BLOCK\n",
    "    print(\"Model Training\")\n",
    "    clf = MLPClassifier(tol=1e-4, \n",
    "                        alpha=1e-4, \n",
    "                        learning_rate=\"adaptive\", \n",
    "                        activation='relu', \n",
    "                        hidden_layer_sizes=(12, 4), \n",
    "                        shuffle=True, \n",
    "                        early_stopping=True, \n",
    "                        max_iter=80)\n",
    "    \n",
    "    clf.fit(train_X, train_y)\n",
    "    # END MODEL TRAINING BLOCK\n",
    "\n",
    "    # BEGIN MODEL EVALUATION BLOCK\n",
    "    print(\"Model Evaluation\")\n",
    "    predict_metrics = [(\"iou\", jaccard_score)]\n",
    "    predict_proba_metrics = [(\"pr_auc\", average_precision_score)]\n",
    "\n",
    "    pred = clf.predict(test_X)\n",
    "    pred_proba = clf.predict_proba(test_X)[:, 1]\n",
    "\n",
    "    metric_results = {}\n",
    "    for key, metric in predict_metrics:\n",
    "        metric_results[key] = metric(test_y, pred)\n",
    "\n",
    "    for key, metric in predict_proba_metrics:\n",
    "        metric_results[key] = metric(test_y, pred_proba)\n",
    "    # END MODEL EVALUATION BLOCK\n",
    "\n",
    "    # BEGIN PRINTING BLOCK\n",
    "    metric_results[\"fitness\"] = opt.get_fitness_()\n",
    "    metric_results[\"feature_count\"] = len(opt.selected_features)\n",
    "    for metric in metric_results.keys():\n",
    "        print(f\"    {metric}: {metric_results[metric]}\")\n",
    "\n",
    "    print(\"-\"*33)\n",
    "    # END PRINTING BLOCK\n",
    "\n",
    "    return metric_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of evaluations:  54\n",
      "---------------------------------\n",
      "START NEW MODEL\n",
      "INFO: 0.01 INDEP: 0.01\n",
      "Feature Optimization\n",
      "Count: 20736 12 4\n",
      "Fitness (Gen 150): 0.9569122435307272\n",
      "Data Preparation\n",
      "Model Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yevhe\\mambaforge\\envs\\swiftt\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:693: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation\n",
      "    iou: 0.5329464181029429\n",
      "    pr_auc: 0.6897380894207212\n",
      "    fitness: 0.9569122435307272\n",
      "    feature_count: 11\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "START NEW MODEL\n",
      "INFO: 0.01 INDEP: 0.1\n",
      "Feature Optimization\n",
      "Count: 20736 12 4\n"
     ]
    }
   ],
   "source": [
    "informativeness_values = [0.01, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "independency_values = [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "\n",
    "print(\"Num of evaluations: \", len(informativeness_values) * len(independency_values))\n",
    "\n",
    "result_matrix = [[None]*len(independency_values) for i in range(len(informativeness_values))]\n",
    "for i in range(len(informativeness_values)):\n",
    "    for j in range(len(independency_values)):\n",
    "        result_matrix[i][j] = evaluateModel(informativeness_values[i], independency_values[j])\n",
    "\n",
    "with open(os.path.join(\"grid_search\", f\"normp4_results.json\"), 'w') as fout:\n",
    "    json.dump(result_matrix, fout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swiftt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
